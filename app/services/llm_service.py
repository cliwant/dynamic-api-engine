"""
LLM ì„œë¹„ìŠ¤
LiteLLMì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ LLM ëª¨ë¸ì„ í˜¸ì¶œí•©ë‹ˆë‹¤.
API ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""
import json
import os
from pathlib import Path
from typing import Optional, Any
from pydantic import BaseModel

try:
    import litellm
    LITELLM_AVAILABLE = True
except ImportError:
    LITELLM_AVAILABLE = False

# gcloud-key.json ê²½ë¡œ ìë™ ì„¤ì •
GCLOUD_KEY_PATH = Path(__file__).parent.parent.parent / "gcloud-key.json"
if GCLOUD_KEY_PATH.exists() and not os.getenv("GOOGLE_APPLICATION_CREDENTIALS"):
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = str(GCLOUD_KEY_PATH)
    print(f"âœ… Vertex AI ì¸ì¦ ì„¤ì •ë¨: {GCLOUD_KEY_PATH}")


class LLMConfig(BaseModel):
    """LLM ì„¤ì •"""
    model: str = "vertex_ai/gemini-2.5-flash"  # ê¸°ë³¸ê°’: Vertex AI Gemini 2.5 Flash
    temperature: float = 0.7
    max_tokens: int = 4000
    top_p: float = 1.0
    frequency_penalty: float = 0.0
    presence_penalty: float = 0.0
    # ì¸ì¦ ì„¤ì •
    api_key: Optional[str] = None
    api_base: Optional[str] = None
    vertex_credentials: Optional[str] = None  # gcloud-key.json ë‚´ìš©
    vertex_project: str = "cliwant-403702"  # í”„ë¡œì íŠ¸ ID
    vertex_location: str = "us-central1"  # ë¦¬ì „


class TableSchema(BaseModel):
    """í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´"""
    table_name: str
    columns: list[dict]
    indexes: list[dict]
    sample_data: list[dict]


class ApiGenerationRequest(BaseModel):
    """API ìƒì„± ìš”ì²­"""
    user_intent: str
    tables: list[TableSchema]
    method: str = "GET"


class GeneratedApiSpec(BaseModel):
    """ìƒì„±ëœ API ìŠ¤í™"""
    path: str
    method: str
    name: str
    description: str
    tags: Optional[str] = None
    logic_type: str
    logic_body: str
    request_spec: dict
    response_spec: Optional[dict] = None
    sample_params: dict
    change_note: str


# LiteLLM ì§€ì› ëª¨ë¸ ëª©ë¡ (ì£¼ìš” í”„ë¡œë°”ì´ë”) - Vertex AIë¥¼ ë¨¼ì € ë°°ì¹˜
# Vertex AI GeminiëŠ” 2.5 ì´ìƒ ë²„ì „ë§Œ ì‚¬ìš© ê°€ëŠ¥ (2.5 ~ 3.0)
SUPPORTED_MODELS = [
    # Vertex AI (Google Cloud) - Gemini 2.5+ ì „ìš©, gcloud-key.json ìë™ ì¸ì¦
    {"id": "vertex_ai/gemini-2.5-flash", "name": "âœ¨ Gemini 2.5 Flash (Vertex)", "provider": "vertex_ai", "auth": "vertex", "default": True},
    {"id": "vertex_ai/gemini-2.5-pro", "name": "ğŸš€ Gemini 2.5 Pro (Vertex)", "provider": "vertex_ai", "auth": "vertex"},
    {"id": "vertex_ai/gemini-2.5-flash-preview-05-20", "name": "Gemini 2.5 Flash Preview (Vertex)", "provider": "vertex_ai", "auth": "vertex"},
    {"id": "vertex_ai/gemini-2.5-pro-preview-05-06", "name": "Gemini 2.5 Pro Preview (Vertex)", "provider": "vertex_ai", "auth": "vertex"},
    # Gemini 3.0 (í–¥í›„ ì¶œì‹œ ì˜ˆì •)
    {"id": "vertex_ai/gemini-3.0-flash", "name": "âš¡ Gemini 3.0 Flash (Vertex)", "provider": "vertex_ai", "auth": "vertex"},
    {"id": "vertex_ai/gemini-3.0-pro", "name": "ğŸŒŸ Gemini 3.0 Pro (Vertex)", "provider": "vertex_ai", "auth": "vertex"},
    # Vertex AI Claude (non-Gemini)
    {"id": "vertex_ai/claude-3-5-sonnet@20241022", "name": "Claude 3.5 Sonnet (Vertex)", "provider": "vertex_ai", "auth": "vertex"},
    {"id": "vertex_ai/claude-3-5-haiku@20241022", "name": "Claude 3.5 Haiku (Vertex)", "provider": "vertex_ai", "auth": "vertex"},
    
    # Google AI (Gemini) - API Key ë°©ì‹
    {"id": "gemini/gemini-2.5-flash", "name": "Gemini 2.5 Flash", "provider": "google", "auth": "api_key"},
    {"id": "gemini/gemini-2.5-pro", "name": "Gemini 2.5 Pro", "provider": "google", "auth": "api_key"},
    {"id": "gemini/gemini-2.5-flash-preview-05-20", "name": "Gemini 2.5 Flash Preview", "provider": "google", "auth": "api_key"},
    
    # OpenAI
    {"id": "gpt-4o", "name": "GPT-4o", "provider": "openai", "auth": "api_key"},
    {"id": "gpt-4o-mini", "name": "GPT-4o Mini", "provider": "openai", "auth": "api_key"},
    {"id": "gpt-4-turbo", "name": "GPT-4 Turbo", "provider": "openai", "auth": "api_key"},
    {"id": "gpt-4", "name": "GPT-4", "provider": "openai", "auth": "api_key"},
    {"id": "gpt-3.5-turbo", "name": "GPT-3.5 Turbo", "provider": "openai", "auth": "api_key"},
    {"id": "o1-preview", "name": "O1 Preview", "provider": "openai", "auth": "api_key"},
    {"id": "o1-mini", "name": "O1 Mini", "provider": "openai", "auth": "api_key"},
    
    # Anthropic
    {"id": "claude-3-5-sonnet-20241022", "name": "Claude 3.5 Sonnet", "provider": "anthropic", "auth": "api_key"},
    {"id": "claude-3-5-haiku-20241022", "name": "Claude 3.5 Haiku", "provider": "anthropic", "auth": "api_key"},
    {"id": "claude-3-opus-20240229", "name": "Claude 3 Opus", "provider": "anthropic", "auth": "api_key"},
    {"id": "claude-3-sonnet-20240229", "name": "Claude 3 Sonnet", "provider": "anthropic", "auth": "api_key"},
    {"id": "claude-3-haiku-20240307", "name": "Claude 3 Haiku", "provider": "anthropic", "auth": "api_key"},
    
    # Azure OpenAI
    {"id": "azure/gpt-4o", "name": "GPT-4o (Azure)", "provider": "azure", "auth": "azure"},
    {"id": "azure/gpt-4", "name": "GPT-4 (Azure)", "provider": "azure", "auth": "azure"},
    {"id": "azure/gpt-35-turbo", "name": "GPT-3.5 Turbo (Azure)", "provider": "azure", "auth": "azure"},
    
    # AWS Bedrock
    {"id": "bedrock/anthropic.claude-3-5-sonnet-20241022-v2:0", "name": "Claude 3.5 Sonnet (Bedrock)", "provider": "bedrock", "auth": "aws"},
    {"id": "bedrock/anthropic.claude-3-haiku-20240307-v1:0", "name": "Claude 3 Haiku (Bedrock)", "provider": "bedrock", "auth": "aws"},
    {"id": "bedrock/amazon.titan-text-express-v1", "name": "Titan Text Express (Bedrock)", "provider": "bedrock", "auth": "aws"},
    
    # Mistral
    {"id": "mistral/mistral-large-latest", "name": "Mistral Large", "provider": "mistral", "auth": "api_key"},
    {"id": "mistral/mistral-medium-latest", "name": "Mistral Medium", "provider": "mistral", "auth": "api_key"},
    {"id": "mistral/mistral-small-latest", "name": "Mistral Small", "provider": "mistral", "auth": "api_key"},
    {"id": "mistral/codestral-latest", "name": "Codestral", "provider": "mistral", "auth": "api_key"},
    
    # Groq
    {"id": "groq/llama-3.3-70b-versatile", "name": "Llama 3.3 70B (Groq)", "provider": "groq", "auth": "api_key"},
    {"id": "groq/llama-3.1-8b-instant", "name": "Llama 3.1 8B (Groq)", "provider": "groq", "auth": "api_key"},
    {"id": "groq/mixtral-8x7b-32768", "name": "Mixtral 8x7B (Groq)", "provider": "groq", "auth": "api_key"},
    
    # Together AI
    {"id": "together_ai/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo", "name": "Llama 3.1 70B (Together)", "provider": "together_ai", "auth": "api_key"},
    {"id": "together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1", "name": "Mixtral 8x7B (Together)", "provider": "together_ai", "auth": "api_key"},
    
    # Ollama (ë¡œì»¬)
    {"id": "ollama/llama3.2", "name": "Llama 3.2 (Ollama)", "provider": "ollama", "auth": "none"},
    {"id": "ollama/mistral", "name": "Mistral (Ollama)", "provider": "ollama", "auth": "none"},
    {"id": "ollama/codellama", "name": "CodeLlama (Ollama)", "provider": "ollama", "auth": "none"},
    
    # OpenRouter
    {"id": "openrouter/openai/gpt-4o", "name": "GPT-4o (OpenRouter)", "provider": "openrouter", "auth": "api_key"},
    {"id": "openrouter/anthropic/claude-3.5-sonnet", "name": "Claude 3.5 Sonnet (OpenRouter)", "provider": "openrouter", "auth": "api_key"},
    {"id": "openrouter/google/gemini-pro-1.5", "name": "Gemini 1.5 Pro (OpenRouter)", "provider": "openrouter", "auth": "api_key"},
]

# ì¸ì¦ ë°©ì‹ ì„¤ëª…
AUTH_METHODS = {
    "api_key": {
        "name": "API Key",
        "description": "API í‚¤ë¥¼ ì§ì ‘ ì…ë ¥í•©ë‹ˆë‹¤.",
        "fields": [{"name": "api_key", "label": "API Key", "type": "password", "required": True}]
    },
    "vertex": {
        "name": "Google Cloud (Vertex AI)",
        "description": "Google Cloud ì„œë¹„ìŠ¤ ê³„ì • JSON í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
        "fields": [
            {"name": "vertex_credentials", "label": "Service Account JSON", "type": "textarea", "required": True},
            {"name": "vertex_project", "label": "Project ID", "type": "text", "required": True},
            {"name": "vertex_location", "label": "Location", "type": "text", "required": False, "default": "us-central1"}
        ]
    },
    "azure": {
        "name": "Azure OpenAI",
        "description": "Azure OpenAI ì—”ë“œí¬ì¸íŠ¸ì™€ API í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
        "fields": [
            {"name": "api_key", "label": "API Key", "type": "password", "required": True},
            {"name": "api_base", "label": "Endpoint URL", "type": "text", "required": True},
            {"name": "api_version", "label": "API Version", "type": "text", "required": False, "default": "2024-02-01"}
        ]
    },
    "aws": {
        "name": "AWS Bedrock",
        "description": "AWS ìê²© ì¦ëª…ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.",
        "fields": [
            {"name": "aws_access_key_id", "label": "Access Key ID", "type": "text", "required": True},
            {"name": "aws_secret_access_key", "label": "Secret Access Key", "type": "password", "required": True},
            {"name": "aws_region", "label": "Region", "type": "text", "required": False, "default": "us-east-1"}
        ]
    },
    "none": {
        "name": "ì¸ì¦ ë¶ˆí•„ìš”",
        "description": "ë¡œì»¬ ëª¨ë¸ (Ollama ë“±)",
        "fields": [{"name": "api_base", "label": "API Base URL", "type": "text", "required": False, "default": "http://localhost:11434"}]
    }
}


def get_supported_models() -> list[dict]:
    """ì§€ì›ë˜ëŠ” LLM ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    return SUPPORTED_MODELS


def get_auth_methods() -> dict:
    """ì¸ì¦ ë°©ì‹ ëª©ë¡ ë°˜í™˜"""
    return AUTH_METHODS


def get_providers() -> list[dict]:
    """í”„ë¡œë°”ì´ë” ëª©ë¡ ë°˜í™˜"""
    providers = {}
    for model in SUPPORTED_MODELS:
        p = model["provider"]
        if p not in providers:
            providers[p] = {"id": p, "auth": model["auth"], "models": []}
        providers[p]["models"].append(model)
    return list(providers.values())


def _build_system_prompt() -> str:
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    return """ë‹¹ì‹ ì€ MySQL API ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì˜ë„ì™€ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆë¥¼ ë¶„ì„í•˜ì—¬ ìµœì ì˜ API ì •ì˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ìƒì„±í•´ì•¼ í•  JSON êµ¬ì¡°:
{
  "path": "API ê²½ë¡œ (ì˜ˆ: users/list, projects/by-type)",
  "method": "HTTP ë©”ì„œë“œ (GET, POST, PUT, DELETE)",
  "name": "API í•œê¸€ ì´ë¦„",
  "description": "API ì„¤ëª…",
  "tags": "íƒœê·¸ (ì²« ë²ˆì§¸ ê²½ë¡œ ì„¸ê·¸ë¨¼íŠ¸)",
  "logic_type": "SQL ë˜ëŠ” MULTI_SQL",
  "logic_body": "ì‹¤í–‰í•  SQL ì¿¼ë¦¬ (:param í˜•ì‹ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ë°”ì¸ë”©)",
  "request_spec": {"param_name": {"type": "string|int|float|bool", "required": true/false, "default": ê¸°ë³¸ê°’, "description": "ì„¤ëª…"}},
  "response_spec": {"type": "list|object", "description": "ì‘ë‹µ ì„¤ëª…"},
  "sample_params": {"param_name": ìƒ˜í”Œê°’},
  "change_note": "ë³€ê²½ ë…¸íŠ¸"
}

ê·œì¹™:
1. SQLì€ ë°˜ë“œì‹œ íŒŒë¼ë¯¸í„° ë°”ì¸ë”©(:param)ì„ ì‚¬ìš©í•˜ì„¸ìš”.
2. í…Œì´ë¸”ì˜ ì¸ë±ìŠ¤ë¥¼ í™œìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ ì¿¼ë¦¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
3. LIMITì™€ OFFSETì„ í†µí•œ í˜ì´ì§€ë„¤ì´ì…˜ì„ ê³ ë ¤í•˜ì„¸ìš”.
4. í•œê¸€ ì´ë¦„ê³¼ ì„¤ëª…ì„ ì‚¬ìš©í•˜ì„¸ìš”.
5. sample_paramsì—ëŠ” ì‹¤ì œ í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜„ì‹¤ì ì¸ ê°’ì„ ë„£ìœ¼ì„¸ìš”.
6. ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."""


def _build_user_prompt(request: ApiGenerationRequest) -> str:
    """ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    tables_info = []
    for table in request.tables:
        table_info = f"""
### í…Œì´ë¸”: {table.table_name}

**ì»¬ëŸ¼:**
{json.dumps(table.columns, indent=2, ensure_ascii=False)}

**ì¸ë±ìŠ¤:**
{json.dumps(table.indexes, indent=2, ensure_ascii=False)}

**ìƒ˜í”Œ ë°ì´í„° (ìµœëŒ€ 5í–‰):**
{json.dumps(table.sample_data[:5], indent=2, ensure_ascii=False, default=str)}
"""
        tables_info.append(table_info)
    
    return f"""ì‚¬ìš©ì ì˜ë„: {request.user_intent}

HTTP ë©”ì„œë“œ: {request.method}

## ì‚¬ìš© ê°€ëŠ¥í•œ í…Œì´ë¸”

{"".join(tables_info)}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì˜ë„ì— ë§ëŠ” API ì •ì˜ JSONì„ ìƒì„±í•´ì£¼ì„¸ìš”."""


def _setup_vertex_auth(config: LLMConfig) -> None:
    """Vertex AI ì¸ì¦ ì„¤ì •"""
    # gcloud-key.json íŒŒì¼ì´ ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ì‚¬ìš©
    if os.getenv("GOOGLE_APPLICATION_CREDENTIALS"):
        return
    
    # vertex_credentialsê°€ ì œê³µëœ ê²½ìš° ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    if config.vertex_credentials:
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            f.write(config.vertex_credentials)
            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = f.name


async def generate_api_spec(
    request: ApiGenerationRequest,
    config: LLMConfig = LLMConfig()
) -> GeneratedApiSpec:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ API ìŠ¤í™ ìƒì„±
    """
    if not LITELLM_AVAILABLE:
        raise ImportError("litellm ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install litellmì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    
    # Vertex AI ì¸ì¦ ì„¤ì •
    if config.vertex_credentials:
        _setup_vertex_auth(config)
    
    system_prompt = _build_system_prompt()
    user_prompt = _build_user_prompt(request)
    
    # LiteLLM í˜¸ì¶œ íŒŒë¼ë¯¸í„° êµ¬ì„±
    completion_kwargs = {
        "model": config.model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "temperature": config.temperature,
        "max_tokens": config.max_tokens,
        "top_p": config.top_p,
    }
    
    # Vertex AI ì„¤ì •
    if config.model.startswith("vertex_ai/"):
        completion_kwargs["vertex_project"] = config.vertex_project
        completion_kwargs["vertex_location"] = config.vertex_location
    
    # API í‚¤/ë² ì´ìŠ¤ ì„¤ì •
    if config.api_key:
        completion_kwargs["api_key"] = config.api_key
    if config.api_base:
        completion_kwargs["api_base"] = config.api_base
    
    try:
        response = await litellm.acompletion(**completion_kwargs)
        
        content = response.choices[0].message.content.strip()
        
        # JSON ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì²˜ë¦¬)
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()
        
        # JSON íŒŒì‹±
        spec_dict = json.loads(content)
        
        return GeneratedApiSpec(**spec_dict)
        
    except json.JSONDecodeError as e:
        raise ValueError(f"LLM ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    except Exception as e:
        raise RuntimeError(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")


def check_llm_availability() -> dict:
    """LLM ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    result = {
        "litellm_installed": LITELLM_AVAILABLE,
        "env_keys": {
            "openai": bool(os.getenv("OPENAI_API_KEY")),
            "anthropic": bool(os.getenv("ANTHROPIC_API_KEY")),
            "google": bool(os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")),
            "vertex": bool(os.getenv("GOOGLE_APPLICATION_CREDENTIALS")),
            "azure": bool(os.getenv("AZURE_API_KEY")),
            "aws": bool(os.getenv("AWS_ACCESS_KEY_ID")),
            "mistral": bool(os.getenv("MISTRAL_API_KEY")),
            "groq": bool(os.getenv("GROQ_API_KEY")),
            "together": bool(os.getenv("TOGETHER_API_KEY")),
            "openrouter": bool(os.getenv("OPENROUTER_API_KEY")),
        }
    }
    
    result["available"] = result["litellm_installed"] and any(result["env_keys"].values())
    
    return result


# ==================== AI ê¸°ëŠ¥ í™•ì¥ ====================

class SqlOptimizationRequest(BaseModel):
    """SQL ìµœì í™” ìš”ì²­"""
    sql_query: str
    table_schemas: list[dict]  # í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´
    indexes: list[dict] = []   # ì‚¬ìš© ê°€ëŠ¥í•œ ì¸ë±ìŠ¤
    execution_time_ms: Optional[float] = None  # í˜„ì¬ ì‹¤í–‰ ì‹œê°„


class SqlOptimizationResult(BaseModel):
    """SQL ìµœì í™” ê²°ê³¼"""
    original_query: str
    optimized_query: str
    suggestions: list[dict]  # [{"type": "INDEX", "message": "...", "priority": "HIGH"}]
    index_recommendations: list[dict]  # ìƒˆ ì¸ë±ìŠ¤ ì œì•ˆ
    explanation: str
    estimated_improvement: Optional[str] = None


class TestCaseGenerationRequest(BaseModel):
    """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„± ìš”ì²­"""
    api_path: str
    method: str
    request_spec: dict
    logic_body: str
    sample_data: list[dict] = []


class TestCase(BaseModel):
    """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤"""
    name: str
    description: str
    params: dict
    expected_behavior: str
    test_type: str  # "positive", "negative", "boundary", "performance"


class TestCaseGenerationResult(BaseModel):
    """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„± ê²°ê³¼"""
    api_path: str
    total_cases: int
    test_cases: list[TestCase]


class NaturalLanguageQueryRequest(BaseModel):
    """ìì—°ì–´ ì¿¼ë¦¬ ìš”ì²­"""
    question: str
    available_apis: list[dict]  # ì‚¬ìš© ê°€ëŠ¥í•œ API ëª©ë¡


class NaturalLanguageQueryResult(BaseModel):
    """ìì—°ì–´ ì¿¼ë¦¬ ê²°ê³¼"""
    question: str
    selected_api: Optional[dict] = None  # ì„ íƒëœ API
    params: dict = {}  # ì¶”ì¶œëœ íŒŒë¼ë¯¸í„°
    confidence: float = 0.0  # ì‹ ë¢°ë„ (0~1)
    explanation: str = ""  # í•´ì„ ì„¤ëª…
    alternative_apis: list[dict] = []  # ëŒ€ì•ˆ API ëª©ë¡


def _build_sql_optimization_prompt(request: SqlOptimizationRequest) -> str:
    """SQL ìµœì í™” í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    return f"""ë‹¹ì‹ ì€ MySQL ì¿¼ë¦¬ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ SQL ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ê³  ì„±ëŠ¥ ê°œì„  ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”.

## ë¶„ì„ ëŒ€ìƒ ì¿¼ë¦¬
```sql
{request.sql_query}
```

## í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ
{json.dumps(request.table_schemas, indent=2, ensure_ascii=False)}

## ì‚¬ìš© ê°€ëŠ¥í•œ ì¸ë±ìŠ¤
{json.dumps(request.indexes, indent=2, ensure_ascii=False)}

{f"## í˜„ì¬ ì‹¤í–‰ ì‹œê°„: {request.execution_time_ms}ms" if request.execution_time_ms else ""}

## ìš”ì²­ì‚¬í•­
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ìµœì í™” ê²°ê³¼ë¥¼ ë°˜í™˜í•´ì£¼ì„¸ìš”:

```json
{{
  "original_query": "ì›ë³¸ ì¿¼ë¦¬",
  "optimized_query": "ìµœì í™”ëœ ì¿¼ë¦¬",
  "suggestions": [
    {{"type": "INDEX|REWRITE|JOIN|LIMIT", "message": "ê°œì„  ì‚¬í•­ ì„¤ëª…", "priority": "HIGH|MEDIUM|LOW"}}
  ],
  "index_recommendations": [
    {{"table": "í…Œì´ë¸”ëª…", "columns": ["ì»¬ëŸ¼1", "ì»¬ëŸ¼2"], "type": "INDEX|UNIQUE|FULLTEXT", "reason": "ì´ìœ "}}
  ],
  "explanation": "ì „ë°˜ì ì¸ ì„¤ëª… (í•œê¸€)",
  "estimated_improvement": "ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ (ì˜ˆ: 50% ê°œì„ )"
}}
```

ê·œì¹™:
1. ì¸ë±ìŠ¤ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ë„ë¡ ì¿¼ë¦¬ ìˆ˜ì •
2. ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì„ íƒ ì œê±°
3. JOIN ìˆœì„œ ìµœì í™”
4. WHERE ì ˆ ì¡°ê±´ ìˆœì„œ ìµœì í™”
5. LIMIT í™œìš© ê¶Œì¥
6. ì„œë¸Œì¿¼ë¦¬ë³´ë‹¤ JOIN ì„ í˜¸"""


def _build_test_case_prompt(request: TestCaseGenerationRequest) -> str:
    """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„± í”„ë¡¬í”„íŠ¸"""
    return f"""ë‹¹ì‹ ì€ API í…ŒìŠ¤íŠ¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ API ì •ì˜ë¥¼ ë¶„ì„í•˜ì—¬ í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

## API ì •ë³´
- ê²½ë¡œ: {request.api_path}
- ë©”ì„œë“œ: {request.method}
- ìš”ì²­ ìŠ¤í™: {json.dumps(request.request_spec, indent=2, ensure_ascii=False)}

## SQL ë¡œì§
```sql
{request.logic_body}
```

## ìƒ˜í”Œ ë°ì´í„°
{json.dumps(request.sample_data[:3], indent=2, ensure_ascii=False, default=str)}

## ìš”ì²­ì‚¬í•­
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:

```json
{{
  "api_path": "API ê²½ë¡œ",
  "total_cases": í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ìˆ˜,
  "test_cases": [
    {{
      "name": "í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ëª…",
      "description": "ì„¤ëª… (í•œê¸€)",
      "params": {{"param1": "value1"}},
      "expected_behavior": "ì˜ˆìƒ ë™ì‘ (í•œê¸€)",
      "test_type": "positive|negative|boundary|performance"
    }}
  ]
}}
```

í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìœ í˜•ë³„ ìµœì†Œ ê°œìˆ˜:
1. positive (ì •ìƒ ì¼€ì´ìŠ¤): 3ê°œ ì´ìƒ
2. negative (ì—ëŸ¬ ì¼€ì´ìŠ¤): 2ê°œ ì´ìƒ - í•„ìˆ˜ íŒŒë¼ë¯¸í„° ëˆ„ë½, ì˜ëª»ëœ íƒ€ì… ë“±
3. boundary (ê²½ê³„ê°’ í…ŒìŠ¤íŠ¸): 2ê°œ ì´ìƒ - ë¹ˆ ë¬¸ìì—´, ìµœëŒ€/ìµœì†Œê°’, íŠ¹ìˆ˜ë¬¸ì ë“±
4. performance (ì„±ëŠ¥ í…ŒìŠ¤íŠ¸): 1ê°œ ì´ìƒ - ëŒ€ëŸ‰ ë°ì´í„° ì¡°íšŒ ë“±

ìƒ˜í”Œ ë°ì´í„°ì˜ ì‹¤ì œ ê°’ì„ í™œìš©í•˜ì—¬ í˜„ì‹¤ì ì¸ í…ŒìŠ¤íŠ¸ íŒŒë¼ë¯¸í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”."""


def _build_natural_language_query_prompt(request: NaturalLanguageQueryRequest) -> str:
    """ìì—°ì–´ ì¿¼ë¦¬ í”„ë¡¬í”„íŠ¸"""
    # API ëª©ë¡ì„ ê°„ëµí•˜ê²Œ ì •ë¦¬
    apis_summary = []
    for api in request.available_apis:
        apis_summary.append({
            "route_id": api.get("route_id"),
            "path": api.get("path"),
            "method": api.get("method"),
            "name": api.get("name"),
            "description": api.get("description", ""),
            "request_spec": api.get("request_spec", {}),
        })
    
    return f"""ë‹¹ì‹ ì€ API ê²€ìƒ‰ ë° íŒŒë¼ë¯¸í„° ì¶”ì¶œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìì—°ì–´ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ APIë¥¼ ì°¾ê³  íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

## ì‚¬ìš©ì ì§ˆë¬¸
"{request.question}"

## ì‚¬ìš© ê°€ëŠ¥í•œ API ëª©ë¡
{json.dumps(apis_summary, indent=2, ensure_ascii=False)}

## ìš”ì²­ì‚¬í•­
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•´ì£¼ì„¸ìš”:

```json
{{
  "question": "ì›ë³¸ ì§ˆë¬¸",
  "selected_api": {{
    "route_id": "ì„ íƒëœ API ID",
    "path": "API ê²½ë¡œ",
    "method": "HTTP ë©”ì„œë“œ"
  }},
  "params": {{"param_name": "ì¶”ì¶œëœê°’"}},
  "confidence": 0.95,
  "explanation": "í•´ì„ ì„¤ëª… (í•œê¸€) - ì™œ ì´ APIë¥¼ ì„ íƒí–ˆê³ , íŒŒë¼ë¯¸í„°ë¥¼ ì–´ë–»ê²Œ ì¶”ì¶œí–ˆëŠ”ì§€",
  "alternative_apis": [
    {{"route_id": "ëŒ€ì•ˆ API ID", "path": "ê²½ë¡œ", "reason": "ì´ APIë„ ì‚¬ìš© ê°€ëŠ¥í•œ ì´ìœ "}}
  ]
}}
```

ê·œì¹™:
1. ì§ˆë¬¸ì—ì„œ ì–¸ê¸‰ëœ í‚¤ì›Œë“œë¡œ ê°€ì¥ ì í•©í•œ APIë¥¼ ì°¾ìœ¼ì„¸ìš”.
2. ì§ˆë¬¸ì—ì„œ íŒŒë¼ë¯¸í„° ê°’ì„ ì¶”ì¶œí•˜ì„¸ìš” (ì˜ˆ: "í™ê¸¸ë™ ì‚¬ìš©ì" â†’ {{"user_name": "í™ê¸¸ë™"}})
3. ìˆ«ì, ë‚ ì§œ, ID ë“±ì„ ìë™ìœ¼ë¡œ ì¸ì‹í•˜ì—¬ íŒŒë¼ë¯¸í„°ì— ë§¤í•‘í•˜ì„¸ìš”.
4. í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ confidenceë¥¼ ë‚®ê²Œ ì„¤ì •í•˜ì„¸ìš”.
5. ì í•©í•œ APIê°€ ì—†ìœ¼ë©´ selected_apië¥¼ nullë¡œ ì„¤ì •í•˜ê³  ì„¤ëª…í•˜ì„¸ìš”.
6. ì—¬ëŸ¬ APIê°€ ê°€ëŠ¥í•˜ë©´ alternative_apisì— ì¶”ê°€í•˜ì„¸ìš”."""


async def optimize_sql(
    request: SqlOptimizationRequest,
    config: LLMConfig = LLMConfig()
) -> SqlOptimizationResult:
    """SQL ì¿¼ë¦¬ ìµœì í™” ì œì•ˆ"""
    if not LITELLM_AVAILABLE:
        raise ImportError("litellm ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    if config.vertex_credentials:
        _setup_vertex_auth(config)
    
    prompt = _build_sql_optimization_prompt(request)
    
    completion_kwargs = {
        "model": config.model,
        "messages": [
            {"role": "system", "content": "ë‹¹ì‹ ì€ MySQL ì¿¼ë¦¬ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3,  # ì¼ê´€ì„± ìˆëŠ” ê²°ê³¼ë¥¼ ìœ„í•´ ë‚®ì€ ì˜¨ë„
        "max_tokens": config.max_tokens,
    }
    
    if config.model.startswith("vertex_ai/"):
        completion_kwargs["vertex_project"] = config.vertex_project
        completion_kwargs["vertex_location"] = config.vertex_location
    
    if config.api_key:
        completion_kwargs["api_key"] = config.api_key
    
    try:
        response = await litellm.acompletion(**completion_kwargs)
        content = response.choices[0].message.content.strip()
        
        # JSON ì¶”ì¶œ
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()
        
        result_dict = json.loads(content)
        return SqlOptimizationResult(**result_dict)
        
    except json.JSONDecodeError as e:
        raise ValueError(f"LLM ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    except Exception as e:
        raise RuntimeError(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")


async def generate_test_cases(
    request: TestCaseGenerationRequest,
    config: LLMConfig = LLMConfig()
) -> TestCaseGenerationResult:
    """API í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìë™ ìƒì„±"""
    if not LITELLM_AVAILABLE:
        raise ImportError("litellm ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    if config.vertex_credentials:
        _setup_vertex_auth(config)
    
    prompt = _build_test_case_prompt(request)
    
    completion_kwargs = {
        "model": config.model,
        "messages": [
            {"role": "system", "content": "ë‹¹ì‹ ì€ API í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.5,
        "max_tokens": config.max_tokens,
    }
    
    if config.model.startswith("vertex_ai/"):
        completion_kwargs["vertex_project"] = config.vertex_project
        completion_kwargs["vertex_location"] = config.vertex_location
    
    if config.api_key:
        completion_kwargs["api_key"] = config.api_key
    
    try:
        response = await litellm.acompletion(**completion_kwargs)
        content = response.choices[0].message.content.strip()
        
        # JSON ì¶”ì¶œ
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()
        
        result_dict = json.loads(content)
        return TestCaseGenerationResult(**result_dict)
        
    except json.JSONDecodeError as e:
        raise ValueError(f"LLM ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    except Exception as e:
        raise RuntimeError(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")


async def process_natural_language_query(
    request: NaturalLanguageQueryRequest,
    config: LLMConfig = LLMConfig()
) -> NaturalLanguageQueryResult:
    """ìì—°ì–´ë¡œ API í˜¸ì¶œ"""
    if not LITELLM_AVAILABLE:
        raise ImportError("litellm ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    if config.vertex_credentials:
        _setup_vertex_auth(config)
    
    prompt = _build_natural_language_query_prompt(request)
    
    completion_kwargs = {
        "model": config.model,
        "messages": [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìì—°ì–´ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì í•©í•œ APIë¥¼ ì°¾ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3,
        "max_tokens": config.max_tokens,
    }
    
    if config.model.startswith("vertex_ai/"):
        completion_kwargs["vertex_project"] = config.vertex_project
        completion_kwargs["vertex_location"] = config.vertex_location
    
    if config.api_key:
        completion_kwargs["api_key"] = config.api_key
    
    try:
        response = await litellm.acompletion(**completion_kwargs)
        content = response.choices[0].message.content.strip()
        
        # JSON ì¶”ì¶œ
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()
        
        result_dict = json.loads(content)
        return NaturalLanguageQueryResult(**result_dict)
        
    except json.JSONDecodeError as e:
        raise ValueError(f"LLM ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    except Exception as e:
        raise RuntimeError(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
